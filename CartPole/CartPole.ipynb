{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPole.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "esG2hlYaRoPW"
      },
      "source": [
        "import numpy as np # used for arrays\n",
        "\n",
        "import gym # pull the environment\n",
        "\n",
        "import time # to get the time\n",
        "\n",
        "import math # needed for calculations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGVW6kr5R8U4"
      },
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "print(env.action_space.n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U03x-RFgR9ay"
      },
      "source": [
        "LEARNING_RATE = 0.1\n",
        "\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 60000\n",
        "total = 0\n",
        "total_reward = 0\n",
        "prior_reward = 0\n",
        "\n",
        "Observation = [30, 30, 50, 50]\n",
        "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
        "\n",
        "epsilon = 1\n",
        "\n",
        "epsilon_decay_value = 0.99995"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Oe4eNvbSEgc"
      },
      "source": [
        "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
        "q_table.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdg-uq7dSGJ9"
      },
      "source": [
        "def get_discrete_state(state):\n",
        "    discrete_state = state/np_array_win_size+ np.array([15,10,1,10])\n",
        "    return tuple(discrete_state.astype(np.int))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk7FcazMSMhu"
      },
      "source": [
        "for episode in range(EPISODES + 1): //go through the episodes\n",
        "    t0 = time.time() //set the initial time\n",
        "    discrete_state = get_discrete_state(env.reset()) //get the discrete start for the restarted environment \n",
        "    done = False\n",
        "    episode_reward = 0 //reward starts as 0 for each episode\n",
        "\n",
        "    if episode % 2000 == 0: \n",
        "        print(\"Episode: \" + str(episode))\n",
        "\n",
        "    while not done: \n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "\n",
        "            action = np.argmax(q_table[discrete_state]) //take cordinated action\n",
        "        else:\n",
        "\n",
        "            action = np.random.randint(0, env.action_space.n) //do a random ation\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action) //step action to get new states, reward, and the \"done\" status.\n",
        "\n",
        "        episode_reward += reward //add the reward\n",
        "\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        if episode % 2000 == 0: //render\n",
        "            env.render()\n",
        "\n",
        "        if not done: //update q-table\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    if epsilon > 0.05: //epsilon modification\n",
        "        if episode_reward > prior_reward and episode > 10000:\n",
        "            epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
        "\n",
        "            if episode % 500 == 0:\n",
        "                print(\"Epsilon: \" + str(epsilon))\n",
        "\n",
        "    t1 = time.time() //episode has finished\n",
        "    episode_total = t1 - t0 //episode total time\n",
        "    total = total + episode_total\n",
        "\n",
        "    total_reward += episode_reward //episode total reward\n",
        "    prior_reward = episode_reward\n",
        "\n",
        "    if episode % 1000 == 0: //every 1000 episodes print the average time and the average reward\n",
        "        mean = total / 1000\n",
        "        print(\"Time Average: \" + str(mean))\n",
        "        total = 0\n",
        "\n",
        "        mean_reward = total_reward / 1000\n",
        "        print(\"Mean Reward: \" + str(mean_reward))\n",
        "        total_reward = 0\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}